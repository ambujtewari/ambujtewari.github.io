<html>
<title>
Stochastic MIrror Descent Algorithm made Sparse (SMIDAS)
</title>
<head>
</head>
<body>

<h2> Overview </h2>

<p>
SMIDAS is a C++ implementation of the stochastic mirror descent algorithm proposed in

<ul>
<li>
	Shai Shalev-Shwartz and Ambuj Tewari, <i> <a href="http://doi.acm.org/10.1145/1553374.1553493">Stochastic methods for l<sub>1</sub> regularized loss minimization.</a> </i>
	Proceedings of the 26th International Conference on Machine Learning, pages 929-936, 2009.
</li>
</ul>
</p>

<p>
It can be used for <i>l<sub>1</sub></i>-regularized loss minimization for both classification and regression problems. This means that it
(approximately) solves the following minimization problem: 
</p>

<p>
min<sub>w</sub> &nbsp;
(1/m) &#8721;<sub><var>i</var>=1, &#8230; , <var>m</var></sub> L(w<sup>T</sup> x<sub><var>i</var></sub>, y<sub><var>i</var></sub>) + &lambda;|w|<sub>1</sub>
</p>

<p>
Currently supported loss functions are the logistic loss [L(a,b) = log(1+exp(-ab))], the hinge loss [L(a,b) = max(0,1-ab)], and the squared loss [L(a,b) = (a-b)<sup>2</sup>]. The x<sub><var>i</var></sub>'s are <i>example vectors</i> with <i>d</i> coordinates x<sub><var>i</var>,1</sub>, &#8230;  x<sub><var>i</var>,d</sub>. The y<sub><var>i</var></sub>'s are <i>labels</i> that are either &plusmn;1 (classification) or arbitrary floats (regression). The logistic and hinge losses only make sense in a classification context but the squared loss can be used in either. &lambda; is the <i>regularization parameter</i>. SMIDAS is designed to run fast even for large values of <var>m</var> and <var>d</var> and can exploit the sparsity in the examples (i.e. if a lot of coordinates in the example vectors are zero).
</p>

<h2> Source Code </h2>

<p>
The source code is released under the GNU General Public License. The authors are not responsible for any consequences arising out of the use of the code, so please use it at your own risk. We do, however, appreciate receiving bug reports from you. Please direct them to <a href="../..">Ambuj Tewari</a> via email.
</p>

<p>
If you use SMIDAS in your research, please cite it as
<ul>
<li>
        Shai Shalev-Shwartz and Ambuj Tewari, <i> <a href="http://doi.acm.org/10.1145/1553374.1553493"> Stochastic methods for l<sub>1</sub> regularized loss minimization.</a> </i>
        Proceedings of the 26th International Conference on Machine Learning, pages 929-936, 2009.
</li>
</ul>
The code was developed under Linux using g++ (GCC) version 3.4.6. Current version of SMIDAS is 1.1. The source code of the current version is available here:
</p>
<p>
<ul>
<li>
<a href="./smidas.tar.gz">
smidas.tar.gz
</a>
</li>
</ul>
</p>

<h2> Installation </h2>

<p>
After downloading the file <tt>smidas.tar.gz</tt> into a suitable directory, unpack it by typing
</p>

<p>
<pre> tar zxvf smidas.tar.gz </pre>
</p>

<p>
This will create a directory called <tt>SMIDAS</tt> with the following 9 files in it:

<ul>
<li><tt> cmd_line.cc </tt></li>
<li><tt> cmd_line.h </tt></li>
<li><tt> cmd_option.h </tt></li>
<li><tt> Losses.h </tt></li>
<li><tt> gpl.txt </tt></li>
<li><tt> Makefile </tt></li>
<li><tt> smidas.cc </tt></li>
<li><tt> test_examples </tt></li>
<li><tt> test_labels </tt></li>
</ul>
</p>

Now, type
<p>
<pre> make </pre>
</p>

<p>
This will create an executable named <tt>smidas</tt> (plus a couple of object files).
</p>

<h2> Usage </h2>

After installing, simply type

<p>
<pre> ./smidas
</pre>
</p>

and the following usage message is displayed:

<pre>

USAGE: ./smidas [options] <instances-file> <labels-file>

SMIDAS -- L_1 regularized loss minimization

OPTIONS:
 -lambda regularization parameter (default = 0.001)
 -iters number of iterations (default = 1000)
 -eta learning rate (default = 0.001)
 -loss 0 for logistic, 1 for hinge, 2 for squared loss (default = 0)
 -printout after how many iterations to print w (default 1000)

</pre>

All 5 command line options are optional and receive their default values indicated above if they are not supplied by the user. The options <tt>lambda, loss</tt> and <tt>iters</tt> set the regularization parameter, the loss function to use, and the number of iterations for which the SMIDAS algorithm will be run, respectively. SMIDAS requires a learning rate parameter &eta;. The option <tt>eta</tt> is meant to supply this. The <tt>printout</tt> option selects the interval after which a summary of current state of the optimization will be printed on the standard output. The following information about the current weight vector w is included in this summary:

<ol>
<li> time elapsed (in milliseconds)</li>
<li> number of times the data matrix was accessed</li>
<li> |w|<sub>1</sub>, the <var>l<sub>1</sub></var>-norm of w</li>
<li> |w|<sub>0</sub>, the <var>l<sub>0</sub></var>-norm (no. of non-zero coordinates) of w</li>
<li> (1/m) &#8721;<sub><var>i</var>=1, &#8230; , <var>m</var></sub> L(w<sup>T</sup> x<sub><var>i</var></sub>, y<sub><var>i</var></sub>), the average loss of w</li>
<li> (1/m) &#8721;<sub><var>i</var>=1, &#8230; , <var>m</var></sub> L(w<sup>T</sup> x<sub><var>i</var></sub>, y<sub><var>i</var></sub>) + &lambda;|w|<sub>1</sub>, the objective function</li>
<li> &#8721;<sub><var>i</var>=1, &#8230; , <var>m</var></sub> 1[w<sup>T</sup> x<sub><var>i</var></sub> y<sub><var>i</var></sub> &#8804; 0], the number of mistakes on the examples (ignore this field if doing regression)
</ol>

<h3> Data File Format </h3>

<p>
Conceptually, we often think of the data as a huge <var>m</var> x <var>d</var> matrix with the examples sitting as rows of this matrix. Viewed this way, <tt>smidas</tt> expects its input data file to be in a <em>sparse row major</em> format as described below:

<pre>
<font color="blue">Line 1:</font> m d
<font color="blue">Line 2:</font> N i1 v1 i2 v2 ...
&#8942;
<font color="blue">Line <var>m</var>+1:</font> N i1 v1 i2 v2 ...
</pre>
</p>

<p>
The first line tells <tt>smidas</tt> that there are <var>m</var> examples each with <var>d</var> features or coordinates.
The second line gives non-zero features for example 1. The number <tt>N</tt> is the total number of non-zero features for that examples. This is followed by N pairs of the form <tt>i1 v1</tt>. The pair <tt>i1 v1</tt> means that feature (i1+1) has value v1.
Note that, following C/C++ convention, <font color="red">the indices <tt>i1, i2, ...</tt> start from zero</font>. For example, if features 1, 3 and 10 are the only non-zero features for example 1, and these have values 0.2, 0.9 and -0.7 respectively, then line 2 would read:

<pre>
3 0 0.2 2 0.9 9 -0.7
</pre>

The example indices need not appear in increasing order.
</p>

<p>Line 3 then gives values for example 2, Line 4 for example 3, and so on till Line <var>m</var>+1.

<h3> Label File Format </h3>

The label file format is pretty simple: Line <var>j</var> gives the label of example <var>j</var>.
So, there should be a total of <var>m</var> lines in the label file and each line should have a single number
(&plusmn;1 in the classification case, any floating point number in the regression case).

<h3> Test Run </h3>

To make sure everything runs okay on your machine, use the files <tt>test_examples</tt> and <tt>test_labels</tt> to do a test run. The file <tt>test_examples</tt> encodes the following 3 x 5 data matrix (3 examples, 5 features):
<pre>
0    0    0    0.2  0.2
0    0    0.3  0   -0.5
0.4  0.6  0    0    0
</pre>
The labels for the 3 examples are 1,1 and -1 respectively.
Now, run <tt>smidas</tt> with logistic loss, the default value (0.001) of the regularization parameter, and learning rate 0.01, by typing

<pre> ./smidas -eta 0.01 -iters 500000 -printout 50000 test_examples test_labels </pre>

This runs <tt>smidas</tt> for 500,000 iterations printing a summary line every 50,000 iterations. You should see an output that looks something like

<pre>
110 200000 21.8226 5 0.110917 0.13274 0
250 400000 28.8088 5 0.0572377 0.0860465 0
380 600000 32.9352 5 0.0384127 0.0713479 0
500 800000 35.8222 5 0.0290536 0.0648759 0
610 1000000 37.9964 5 0.0234737 0.0614701 0
730 1200000 39.7255 5 0.0197936 0.0595191 0
850 1400000 41.1593 5 0.0171696 0.0583289 0
960 1600000 42.37 5 0.0152157 0.0575857 0
1080 1800000 43.405 5 0.0137119 0.057117 0
1210 2000000 44.3091 5 0.0125135 0.0568226 0
</pre>

The actual output might be slightly different due to the randomness in the algorithm but the objective function should converge to <tt>0.0551</tt> for this toy example.

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-11903002-1");
pageTracker._trackPageview();
} catch(err) {}</script>


</body>
</html>
