<html><head><title>STATS 605, Winter 2013</title><style type="text/css">ol{margin:0;padding:0}.c11{vertical-align:top;width:180pt;border-style:solid;border-color:#000000;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c0{padding-left:0pt;line-height:1.0;direction:ltr;margin-left:36pt}.c20{list-style-type:circle;margin:0;padding:0}.c4{padding-left:0pt;direction:ltr;margin-left:36pt}.c2{color:#222222;background-color:#ffffff;font-weight:bold}.c3{list-style-type:disc;margin:0;padding:0}.c6{height:11pt;direction:ltr}.c22{color:inherit;text-decoration:inherit}.c23{max-width:540pt;padding:36pt 36pt 36pt 36pt}.c17{color:#1155cc;text-decoration:underline}.c5{color:#222222;background-color:#ffffff}.c13{padding-left:0pt;margin-left:72pt}.c18{color:#333333}.c14{font-weight:bold}.c9{line-height:1.0}.c7{direction:ltr}.c8{text-align:center}.c24{border-collapse:collapse}.c19{font-size:14pt}.c10{margin-left:18pt}.c15{font-size:18pt}.c1{height:0pt}.c12{background-color:#ffffff}.c21{color:#ff0000}.c16{font-style:italic}.title{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:36pt;font-family:"Arial";font-weight:bold;padding-bottom:6pt}.subtitle{padding-top:18pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:24pt;font-family:"Georgia";padding-bottom:4pt}li{color:#000000;font-size:11pt;font-family:"Arial"}p{color:#000000;font-size:11pt;margin:0;font-family:"Arial"}h1{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:18pt;font-family:"Arial";font-weight:bold;padding-bottom:6pt}h2{padding-top:18pt;line-height:1.15;text-align:left;color:#000000;font-size:14pt;font-family:"Arial";font-weight:bold;padding-bottom:4pt}h3{padding-top:14pt;line-height:1.15;text-align:left;color:#666666;font-size:12pt;font-family:"Arial";font-weight:bold;padding-bottom:4pt}h4{padding-top:12pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:"Arial";padding-bottom:2pt}h5{padding-top:11pt;line-height:1.15;text-align:left;color:#666666;font-size:10pt;font-family:"Arial";font-weight:bold;padding-bottom:2pt}h6{padding-top:10pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:10pt;font-family:"Arial";padding-bottom:2pt}</style></head><body class="c12 c23"><p class="c7 c8"><span class="c14 c15">STATS 605: </span><span class="c14 c15 c12 c18">Advanced Topics in Modeling and Data Analysis</span></p><p class="c6 c8"><span class="c14 c19"></span></p><p class="c7 c8"><span class="c14 c19">Winter 2013</span></p><p class="c6 c8"><span class="c14 c16 c19"></span></p><p class="c7 c8"><span class="c14 c19 c16">High Dimensional Statistics</span></p><p class="c6 c8"><span></span></p><p class="c7 c10"><span class="c17"><a class="c22" href="#h.fvkcwsi3rz3v">Class Information</a></span></p><p class="c7 c10"><span class="c17"><a class="c22" href="#h.eeh0xgr76d2i">Instructor Information</a></span></p><p class="c7 c10"><span class="c17"><a class="c22" href="#h.tt9bywh041jk">Grading</a></span></p><p class="c7 c10"><span class="c17"><a class="c22" href="#h.3hijhum2mdu7">Topics</a></span></p><p class="c7 c10"><span class="c17"><a class="c22" href="#h.5p9xu3vzug5d">Schedule</a></span></p><p class="c7 c10"><span class="c17"><a class="c22" href="#h.9c2yn6g5gfgu">Readings</a></span></p><h3 class="c7"><a name="h.fvkcwsi3rz3v"></a><span>Class Information</span></h3><p class="c6"><span></span></p><ol class="c3" start="1"><li class="c4"><span class="c14">Days &amp; Time:</span><span>&nbsp;Mondays and Wednesdays, 4-5:30pm</span></li><li class="c4"><span class="c14">Location:</span><span>&nbsp;</span><span class="c17"><a class="c22" href="https://rooms.lsa.umich.edu/classrooms/DENN130">130 Dennison</a></span></li><li class="c4"><span class="c14">Description:</span><span>&nbsp;This advanced level graduate course will focus on computational and statistical issues that arise when dealing with high dimensional problems (roughly speaking, problems where the number of parameters to be estimated exceeds the number of samples available) in regression, classification, matrix completion, covariance estimation, clustering, and graphical model estimation. Such problems arise in diverse application areas such as genomics, finance, neuroscience, tomography, climatology, and recommender systems. We will also explore connections to concentration inequalities, random matrix theory, and convex geometry.</span></li><li class="c4"><span class="c14">Textbook:</span><span>&nbsp;No textbook is required for this course.</span></li><li class="c4"><span class="c14">Ctools</span><span>: You should access the </span><span class="c17"><a class="c22" href="https://ctools.umich.edu/portal">Ctools</a></span><span>&nbsp;class page for this course frequently. It will contain important announcements, posted homework assignments, archive of emails to the class, etc.</span></li></ol><h3 class="c7"><a name="h.eeh0xgr76d2i"></a><span>Instructor Information</span></h3><p class="c7"><span class="c14">Name:</span><span>&nbsp;Ambuj Tewari</span></p><p class="c7"><span class="c14">Office:</span><span>&nbsp;454 West Hall</span></p><p class="c7"><span class="c14">Office Hours:</span><span>&nbsp;By appointment</span></p><p class="c7"><span class="c14">Email:</span><span>&nbsp;</span><span class="c17"><a class="c22" href="mailto:tewaria@umich.edu">tewaria@umich.edu</a></span></p><h3 class="c7"><a name="h.tt9bywh041jk"></a><span>Grading</span></h3><p class="c7"><span>The final grade in the course will be determined by your scores in 4 homeworks and one final project using the weights given below.</span></p><p class="c6"><span></span></p><ol class="c3" start="1"><li class="c4"><span class="c14">Homeworks (60%):</span><span>&nbsp;There will be 4 equally weighted homework assignments. The problems on the homeworks will be a mix of theoretical and programming problems. There is no &ldquo;official&rdquo; programming language for the course. Knowledge of either Matlab or R should be sufficient to solve problems.</span></li><li class="c4"><span class="c14">Final Project (40%):</span><span>&nbsp;An important component of the course is a final project which can either be:</span></li></ol><ol class="c20" start="1"><li class="c13 c7"><span>a </span><span class="c16">survey</span><span>&nbsp;of some actively developing sub-topic within high dimensional statistics, or</span></li><li class="c13 c7"><span>a </span><span class="c16">research project</span><span>&nbsp;involving contributing novel research (theoretical result, statistical method, computational algorithm, or interesting application) to the area of high dimensional statistics</span></li></ol><ol class="c3" start="3"><li class="c4"><span class="c14">Project credit breakup: </span><span>Surveys have to be written individually. However, teams of up to 2 students can be formed for a research project. To get full credit, surveys have to be very high quality: they should be similar to a publishable survey article in a top journal. The bar for research projects will be lower because of the time constraint and the inherent uncertainty in the research process. While you&rsquo;re not required to deliver publication quality research work by the end of the semester, you&rsquo;re encouraged to do so. I will provide some suggestions for research projects but you should feel free to work on any problem that interests you. The 40% credit for the project will be split as follows:</span></li></ol><ol class="c20" start="1"><li class="c13 c7"><span>Project proposal (5%)</span></li><li class="c13 c7"><span>Project presentation in class (10%)</span></li><li class="c13 c7"><span>Project final report (25%)</span></li></ol><h3 class="c7"><a name="h.3hijhum2mdu7"></a><span>Topics</span></h3><p class="c7"><span>A tentative list of topics is:</span></p><ol class="c3" start="1"><li class="c4"><span class="c14">Regression:</span><span>&nbsp;Lasso, group lasso, theoretical guarantees, optimization algorithms, compressed sensing, phase transitions</span></li><li class="c4"><span class="c14">Classification:</span><span>&nbsp;Performance in high dimensions of one or more of the following methods: Fisher linear discriminant, Bayes classifiers, distance-based classifiers, regularized loss minimization based classifiers</span></li><li class="c4"><span class="c14">Matrix completion: </span><span>Trace norm regularization, Non-commutative versions of concentration inequalities (Hoeffding&rsquo;s, Bernstein&rsquo;s)</span></li><li class="c4"><span class="c14">Clustering: </span><span>k-means, subspace clustering</span></li><li class="c4"><span class="c14">Large covariance matrices: </span><span>PCA, covariance estimation, precision matrix estimation</span></li><li class="c4"><span class="c14">High dimensional graphical models: </span><span>Ising models and Gaussian graphical models</span></li></ol><h3 class="c7"><a name="h.5p9xu3vzug5d"></a><span>Schedule</span></h3><p class="c6"><span></span></p><a href="#" name="158e7b844d7d126ee8d633a069f474bc8f69d7ce"></a><a href="#" name="0"></a><table cellpadding="0" cellspacing="0" class="c24"><tbody><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span class="c14">Lecture number</span></p></td><td class="c11 c12"><p class="c9 c7 c8"><span class="c14">Day</span></p></td><td class="c11 c12"><p class="c9 c7 c8"><span class="c14">Topics</span></p></td></tr><tr class="c1"><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 9</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>NO CLASS</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>1</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 14</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Course logistics</span></li><li class="c0"><span>Linear model with n &lt;&lt; p, sparsity</span></li><li class="c0"><span>Lasso</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>2</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 16</span></p></td><td class="c11 c12"><ol class="c3" start="4"><li class="c0"><span>l-1 regularized logistic regression</span></li><li class="c0"><span>GLMs with l-1 regularization</span></li><li class="c0"><span>l-1 SVMs, margins</span></li><li class="c0"><span>Matrix completion and trace norm</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 21</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>NO CLASS (MLK Jr. Day)</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>3</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 23</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>1-bit matrix completion</span></li><li class="c0"><span>Non-negative matrix completion</span></li><li class="c0"><span>Covariance and concentration/precision matrix estimation</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>4</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 28</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Gaussian graphical model selection using l_1 regularized log det</span></li><li class="c0"><span>Gaussian graphical model selection using parallel l_1 regularized regressions</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>5</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Jan 30</span></p></td><td class="c11 c12"><ol class="c3" start="3"><li class="c0"><span>High dimensional Ising model selection</span></li><li class="c0"><span>Sparse PCA: ScoTLASS, SDP relaxation of l_0 constrained PCA</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>6</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 4</span></p></td><td class="c11 c12"><ol class="c3" start="5"><li class="c0"><span>Sparse PCA: Generalized power method</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>7</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 6</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>High dimensional k-means</span></li></ol></td></tr><tr class="c1"><td class="c11"><p class="c9 c7 c8"><span>8</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 11</span></p></td><td class="c11 c12"><ol class="c3" start="2"><li class="c0"><span>Sparse subspace clustering</span></li><li class="c0"><span class="c21">HW 1 out</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>9</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 13</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Lasso l_2 error bounds for the linear model case</span></li><li class="c0"><span>Restricted eigenvalue (RE) condition</span></li><li class="c0"><span class="c21">Initial project proposals due (deadline extended till Feb 15)</span></li></ol></td></tr><tr><td class="c11"><p class="c7 c8 c9"><span>10</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 18</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Sup (i.e. l_\infty) norm error bounds and sign consistency of lasso</span></li><li class="c0"><span>Mutual incoherence condition</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>11</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 20</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Sup norm error bound continued</span></li><li class="c0"><span>When does the RE condition hold with high probability? </span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>12</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 25</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Proximal methods</span></li><li class="c0"><span>Examples of prox operators</span></li><li class="c0"><span class="c21">HW 1 due</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>13</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Feb 27</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Convergence rates for proximal methods</span></li><li class="c0"><span class="c21">Final project proposals due</span></li></ol></td></tr><tr><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 4</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>NO CLASS (Spring break)</span></li></ol></td></tr><tr><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 6</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>NO CLASS (Spring break)</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>14</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 11</span></p></td><td class="c11 c12"><ol class="c3" start="4"><li class="c0"><span>Coordinate descent methods</span></li><li class="c0"><span class="c21">HW 2 out</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>15</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 13</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Least Angle Regression (LARS)</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>16</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 18</span></p></td><td class="c11 c12"><ol class="c3" start="2"><li class="c0"><span>LARS: Lasso modification</span></li><li class="c0"><span>Estimation of high dimensional low rank matrices</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>17</span></p></td><td class="c11"><p class="c9 c7"><span>Mar 20</span></p></td><td class="c11"><ol class="c3" start="1"><li class="c0"><span>Estimation of low rank matrices: Decomposition lemma for error matrix</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>18</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 25</span></p></td><td class="c11 c12"><ol class="c3" start="4"><li class="c0"><span>Estimation of low rank matrices: Restricted Strong Convexity (RSC)</span></li><li class="c0"><span class="c21">HW 2 due</span></li><li class="c0"><span class="c21">HW 3 out</span></li></ol></td></tr><tr><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Mar 27</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>NO CLASS</span></li><li class="c0"><span>Attend Prof. Andrew Gelman&rsquo;s talk: &ldquo;Causality and Statistical Learning&rdquo; in the Ford School of Public Policy</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>19</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 1</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Bound on the maximum singular value of a matrix with iid (multivariate normal) rows</span></li><li class="c0"><span>Gordon&rsquo;s Theorem</span></li><li class="c0"><span class="c21">HW 3 due</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>20</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 3</span></p></td><td class="c11 c12"><ol class="c3" start="4"><li class="c0"><span>Proof of Gordon&rsquo;s theorem using Slepian&rsquo;s inequality</span></li><li class="c0"><span>Gaussian concentration inequality for Lipschitz functions</span></li><li class="c0"><span class="c21">HW 4 out</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>21</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 8</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Fisher&rsquo;s LDA in high dimensions</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>22</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 10</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Naive Bayes or Independence Rule in high dimensions</span></li><li class="c0"><span class="c21">HW 4 due</span></li></ol></td></tr><tr><td class="c11"><p class="c9 c7 c8"><span>23</span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 15</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span>Loss based classification in high dimensions</span></li></ol></td></tr><tr><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 17</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span class="c21">Project presentations I</span></li></ol><ol class="c20" start="1"><li class="c13 c9 c7"><span>Hossein</span></li><li class="c9 c7 c13"><span>Yuan</span></li><li class="c13 c9 c7"><span>Can</span></li><li class="c13 c9 c7"><span>Robert</span></li><li class="c13 c9 c7"><span>Phoenix</span></li></ol></td></tr><tr><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11 c12"><p class="c9 c7"><span>Apr 22</span></p></td><td class="c11 c12"><ol class="c3" start="1"><li class="c0"><span class="c21">Project presentations II</span></li></ol><ol class="c20" start="1"><li class="c13 c9 c7"><span>Kam, Yiwei</span></li><li class="c13 c9 c7"><span>Sougata</span></li><li class="c13 c9 c7"><span>Naveen</span></li><li class="c13 c9 c7"><span>Chia Chye Yee</span></li><li class="c13 c9 c7"><span>Xuan</span></li></ol></td></tr><tr><td class="c11"><p class="c6 c9 c8"><span></span></p></td><td class="c11"><p class="c9 c7"><span>Apr 26</span></p></td><td class="c11"><ol class="c3" start="1"><li class="c0"><span class="c21">Project reports due</span></li></ol></td></tr></tbody></table><p class="c6"><span></span></p><h3 class="c7"><a name="h.9c2yn6g5gfgu"></a><span>Readings</span></h3><p class="c7"><span class="c14">Books</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Peter B&uuml;hlmann and Sara Van De Geer. </span><span class="c5 c16">Statistics for High-Dimensional Data: Methods, Theory and Applications</span><span class="c5">. Springer, 2011.</span></li><li class="c4"><span class="c5">Tony Cai and Xiaotong Shen. </span><span class="c5 c16">High-dimensional data analysis</span><span class="c5">. World Scientific, 2010.</span></li><li class="c4"><span>Yonina C. Eldar and Gitta Kutyniok. </span><span class="c16">Compressed Sensing: Theory and Applications. </span><span>Cambridge University Press, 2012.</span></li></ol><p class="c6"><span></span></p><p class="c7"><span class="c14">Overviews</span></p><p class="c6"><span class="c5"></span></p><ol class="c3" start="1"><li class="c4"><span class="c12">Sahand N. Negahban, Pradeep Ravikumar, Martin J. Wainwright, and Bin Yu. A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers. Statist. Sci. Volume 27, Number 4 (2012), 538-557.</span></li><li class="c4"><span class="c5">Wasserman, Larry. &quot;Low Assumptions, High Dimensions.&quot; </span><span class="c5 c16">Rationality, Markets and Morals</span><span class="c5">&nbsp;2.49 (2011).</span></li><li class="c4"><span class="c5">Bruckstein, Alfred M., David L. Donoho, and Michael Elad. &quot;From sparse solutions of systems of equations to sparse modeling of signals and images.&quot; </span><span class="c5 c16">SIAM review</span><span class="c5">&nbsp;51.1 (2009): 34-81.</span></li><li class="c4"><span class="c12">Johnstone, I. M. (2007). High dimensional statistical inference and random matrices. In </span><span class="c12 c16">International Congress of Mathematicians</span><span class="c12">&nbsp;</span><span class="c14 c12">I</span><span class="c12">&nbsp;307&ndash;333. Eur. Math. Soc., Z&uuml;rich.</span></li><li class="c4"><span class="c5">Donoho, David L. &quot;High-dimensional data analysis: The curses and blessings of dimensionality.&quot; </span><span class="c5 c16">AMS Math Challenges Lecture</span><span class="c5">&nbsp;(2000): 1-32.</span></li></ol><p class="c6"><span class="c5"></span></p><p class="c7"><span class="c2">Regression</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Bickel, Peter J., Ya&rsquo;acov Ritov, and Alexandre B. Tsybakov. &quot;Simultaneous analysis of Lasso and Dantzig selector.&quot; </span><span class="c5 c16">The Annals of Statistics</span><span class="c5">&nbsp;37.4 (2009): 1705-1732.</span></li><li class="c4"><span class="c5">Van de Geer, Sara A. &quot;High-dimensional generalized linear models and the Lasso.&quot; </span><span class="c5 c16">The Annals of Statistics</span><span class="c5">&nbsp;36.2 (2008): 614-645.</span></li><li class="c4"><span class="c5">Tibshirani, Robert. &quot;Regression shrinkage and selection via the lasso.&quot; </span><span class="c5 c16">Journal of the Royal Statistical Society. Series B (Methodological)</span><span class="c5">&nbsp;(1996): 267-288.</span></li></ol><p class="c6"><span class="c5"></span></p><p class="c7"><span class="c2">Classification</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Fan, J, Fan, Y. and Wu, Y. (2011). High-dimensional classification. In High-dimensional Data Analysis. (Cai, T.T. and Shen, X., eds.), 3-37, World Scientific, New Jersey.</span></li><li class="c4"><span class="c5">Zhu, J., Rosset, S., Hastie, T., &amp; Tibshirani, R. &quot;1-norm support vector machines.&quot; </span><span class="c5 c16">Advances in neural information processing systems</span><span class="c5">&nbsp;16.1 (2004): 49-56.</span></li><li class="c4"><span class="c5">Bradley, Paul S., and Olvi L. Mangasarian. &quot;Feature selection via concave minimization and support vector machines.&quot; </span><span class="c5 c16">Machine Learning Proceedings of the Fifteenth International Conference (ICML&rsquo;98)</span><span class="c5">. 1998.</span></li></ol><p class="c6"><span class="c5"></span></p><p class="c7"><span class="c2">Low Rank Matrix Estimation, Matrix Completion</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Davenport, M. A., Plan, Y., Berg, E. V. D., &amp; Wootters, M. (2012). 1-Bit Matrix Completion. </span><span class="c5 c16">arXiv preprint arXiv:1209.3672</span><span class="c5">.</span></li><li class="c4"><span class="c5">Negahban, S., &amp; Wainwright, M. J. (2012). Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. </span><span class="c5 c16">The Journal of Machine Learning Research</span><span class="c5">, </span><span class="c5 c16">98888</span><span class="c5">, 1665-1697.</span></li><li class="c4"><span class="c5">Negahban, S., &amp; Wainwright, M. J. (2011). Estimation of (near) low-rank matrices with noise and high-dimensional scaling. </span><span class="c5 c16">The Annals of Statistics</span><span class="c5">,</span><span class="c5 c16">39</span><span class="c5">(2), 1069-1097.</span></li><li class="c4"><span class="c5">Cand&egrave;s, Emmanuel J., and Terence Tao. &quot;The power of convex relaxation: Near-optimal matrix completion.&quot; </span><span class="c5 c16">Information Theory, IEEE Transactions on </span><span class="c5">56.5 (2010): 2053-2080.</span></li></ol><p class="c6"><span></span></p><p class="c7"><span class="c14">Covariance Estimation</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Rothman, A. J., Bickel, P. J., Levina, E., &amp; Zhu, J. (2008). Sparse permutation invariant covariance estimation. </span><span class="c5 c16">Electronic Journal of Statistics</span><span class="c5">, </span><span class="c5 c16">2</span><span class="c5">, 494-515.</span></li><li class="c4"><span class="c5">Bickel, P. J., &amp; Levina, E. (2008). Regularized estimation of large covariance matrices. </span><span class="c5 c16">The Annals of Statistics</span><span class="c5">, </span><span class="c5 c16">36</span><span class="c5">(1), 199-227.</span></li></ol><p class="c6"><span class="c14"></span></p><p class="c7"><span class="c14">Sparse PCA</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Journ&eacute;e, M., Nesterov, Y., Richt&aacute;rik, P., &amp; Sepulchre, R. (2010). Generalized power method for sparse principal component analysis. </span><span class="c5 c16">The Journal of Machine Learning Research</span><span class="c5">, </span><span class="c5 c16">11</span><span class="c5">, 517-553.</span></li><li class="c4"><span class="c5">d&#39;Aspremont, A., El Ghaoui, L., Jordan, M. I., &amp; Lanckriet, G. R. (2007). A direct formulation for sparse PCA using semidefinite programming. </span><span class="c5 c16">SIAM review</span><span class="c5">, </span><span class="c5 c16">49</span><span class="c5">(3), 434-448.</span></li><li class="c4"><span class="c5">Zou, H., Hastie, T., &amp; Tibshirani, R. (2006). Sparse principal component analysis. </span><span class="c5 c16">Journal of computational and graphical statistics</span><span class="c5">, </span><span class="c5 c16">15</span><span class="c5">(2), 265-286.</span></li></ol><p class="c6"><span class="c14"></span></p><p class="c7"><span class="c14">Graphical Models</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Ravikumar, P., Wainwright, M. J., Raskutti, G., &amp; Yu, B. (2011). High-dimensional covariance estimation by minimizing &#8467;1-penalized log-determinant divergence. </span><span class="c5 c16">Electronic Journal of Statistics</span><span class="c5">, </span><span class="c5 c16">5</span><span class="c5">, 935-980.</span></li><li class="c4"><span class="c5">Ravikumar, Pradeep, Martin J. Wainwright, and John D. Lafferty. &quot;High-dimensional Ising model selection using &#8467;1-regularized logistic regression.&quot; </span><span class="c5 c16">The Annals of Statistics</span><span class="c5">&nbsp;38.3 (2010): 1287-1319.</span></li><li class="c4"><span class="c5">Friedman, J., Hastie, T., &amp; Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. </span><span class="c5 c16">Biostatistics</span><span class="c5">, </span><span class="c5 c16">9</span><span class="c5">(3), 432-441.</span></li><li class="c4"><span class="c5">Meinshausen, Nicolai, and Peter B&uuml;hlmann. &quot;High-dimensional graphs and variable selection with the lasso.&quot; </span><span class="c5 c16">The Annals of Statistics</span><span class="c5">&nbsp;34.3 (2006): 1436-1462.</span></li></ol><p class="c6"><span></span></p><p class="c7"><span class="c14">Clustering</span></p><ol class="c3" start="1"><li class="c4"><span class="c5">Bouveyron, C., &amp; Brunet-Saumard, C. (2012). Model-based clustering of high-dimensional data: A review. </span><span class="c5 c16">Computational Statistics &amp; Data Analysis</span><span class="c5">.</span></li><li class="c4"><span class="c5">Sun, W., Wang, J., &amp; Fang, Y. (2012). Regularized k-means clustering of high-dimensional data and its asymptotic consistency. </span><span class="c5 c16">Electronic Journal of Statistics</span><span class="c5">, </span><span class="c5 c16">6</span><span class="c5">, 148-167.</span></li><li class="c4"><span class="c5">Vidal, R. (2011). Subspace clustering. </span><span class="c5 c16">Signal Processing Magazine, IEEE</span><span class="c5">,</span><span class="c5 c16">28</span><span class="c5">(2), 52-68.</span></li><li class="c4"><span class="c5">Witten, D. M., &amp; Tibshirani, R. (2010). A framework for feature selection in clustering. </span><span class="c5 c16">Journal of the American Statistical Association</span><span class="c5">, </span><span class="c5 c16">105</span><span class="c5">(490), 713-726.</span></li></ol><p class="c6"><span></span></p><p class="c7"><span class="c14">Optimization</span></p><ol class="c3" start="1"><li class="c4"><span class="c12">F. Bach, R. Jenatton, J. Mairal, G. Obozinski. Optimization with sparsity-inducing penalties. </span><span class="c12 c16">Foundations and Trends in Machine Learning, </span><span class="c12">4(1):1-106, 2012.</span></li><li class="c4"><span class="c5">Agarwal, A., Negahban, S. N., &amp; Wainwright, M. J. (2011). Fast global convergence of gradient methods for high-dimensional statistical recovery.</span><span class="c5 c16">arXiv preprint arXiv:1104.4824</span><span class="c5">.</span></li></ol></body></html>